import cv2
import numpy as np

# ==========================================
# ‚ö° FAST YOLO (OpenCV DNN - No Masks)
# ==========================================
class YOLODetector:
    def __init__(self, model_path, task=None):
        # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ ONNX + OpenCV DNN ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ö‡∏ô Pi
        if not model_path.endswith('.onnx'):
             print(f"‚ö†Ô∏è Warning: {model_path} is not ONNX. It will be slow!")
        
        print(f"‚ö° Loading Optimized YOLO: {model_path}...")
        self.net = cv2.dnn.readNetFromONNX(model_path)
        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
        
        # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Input Size (320x320 ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î)
        self.input_size = (320, 320)

    def detect(self, frame, conf=0.5, iou=0.4, **kwargs):
        # 1. Prepare Input
        # swapRB=True ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ OpenCV DNN ‡∏ä‡∏≠‡∏ö BGR->RGB
        blob = cv2.dnn.blobFromImage(frame, 1/255.0, self.input_size, swapRB=True, crop=False)
        self.net.setInput(blob)
        
        # 2. Inference (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)
        out = self.net.forward()
        
        # 3. Post-Process (‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
        predictions = np.squeeze(out)
        # ‡∏ñ‡πâ‡∏≤ Shape ‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏´‡∏°‡∏∏‡∏ô‡∏Å‡∏•‡∏±‡∏ö
        if predictions.ndim == 2 and predictions.shape[0] < predictions.shape[1]:
            predictions = predictions.transpose()
            
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ Confidence (Vectorized - ‡πÄ‡∏£‡πá‡∏ß)
        if predictions.shape[1] > 4:
            scores = np.max(predictions[:, 4:], axis=1)
            keep = scores >= conf
            predictions = predictions[keep]
            scores = scores[keep]
        else:
            return [] # ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

        if len(predictions) == 0:
            return []

        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Box [x, y, w, h] ‡πÅ‡∏ö‡∏ö Pixel
        h_img, w_img = frame.shape[:2]
        scale_x = w_img / self.input_size[0]
        scale_y = h_img / self.input_size[1]
        
        boxes = []
        confidences = []
        
        for i, pred in enumerate(predictions):
            cx, cy, w, h = pred[0], pred[1], pred[2], pred[3]
            left = int((cx - w/2) * scale_x)
            top = int((cy - h/2) * scale_y)
            width = int(w * scale_x)
            height = int(h * scale_y)
            boxes.append([left, top, width, height])
            confidences.append(float(scores[i]))

        # NMS (‡∏•‡∏î‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ã‡πâ‡∏≠‡∏ô)
        indices = cv2.dnn.NMSBoxes(boxes, confidences, conf, iou)
        
        results = []
        if len(indices) > 0:
            for i in indices.flatten():
                # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô ultralytics format (Box object) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏Å‡πà‡∏≤‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
                # ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô list [x1, y1, x2, y2] ‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤
                x, y, w, h = boxes[i]
                x1, y1 = max(0, x), max(0, y)
                x2, y2 = min(w_img, x+w), min(h_img, y+h)
                results.append((x1, y1, x2, y2))
                
        return results

    def get_crop(self, img, box, mask_data=None):
        # üöÄ FAST CROP: ‡∏ï‡∏±‡∏î‡∏£‡∏∞‡∏ö‡∏ö Mask ‡∏ó‡∏¥‡πâ‡∏á! (Mask ‡∏Å‡∏¥‡∏ô CPU ‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å)
        # ‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡∏î‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡πÄ‡∏•‡∏¢ ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 10 ‡πÄ‡∏ó‡πà‡∏≤
        if isinstance(box, (list, tuple, np.ndarray)):
            x1, y1, x2, y2 = box
        else:
            # ‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Object ‡∏Ç‡∏≠‡∏á ultralytics (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ß‡πâ)
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
            
        return img[y1:y2, x1:x2]

# ==========================================
# ‚ö° FAST SIFT (Reduced Features)
# ==========================================
class SIFTIdentifier:
    def __init__(self):
        print("‚è≥ Initializing Optimized SIFT...")
        # üöÄ ‡∏•‡∏î nfeatures ‡∏à‡∏≤‡∏Å 2000 ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 800 (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô 2.5 ‡πÄ‡∏ó‡πà‡∏≤)
        self.sift = cv2.SIFT_create(nfeatures=800) 
        
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=30) # ‡∏•‡∏î checks ‡∏à‡∏≤‡∏Å 50 ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 30
        self.flann = cv2.FlannBasedMatcher(index_params, search_params)
        
        # CLAHE
        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))

    def extract_features(self, img_bgr):
        if img_bgr is None or img_bgr.size == 0: return [], None
        
        if len(img_bgr.shape) == 3:
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        else:
            gray = img_bgr
            
        gray = self.clahe.apply(gray)
        kp, des = self.sift.detectAndCompute(gray, None)
        return kp, des

    def compare(self, query_pack, db_pack):
        kp_q, des_q = query_pack
        kp_db, des_db = db_pack
        
        if des_q is None or des_db is None or len(des_q) < 2 or len(des_db) < 2:
            return 0
            
        try:
            matches = self.flann.knnMatch(des_q, des_db, k=2)
        except: return 0

        good_matches = []
        for m, n in matches:
            # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏•‡∏á‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ (0.7 -> 0.75) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
            if m.distance < 0.75 * n.distance:
                good_matches.append(m)
        
        # üöÄ OPTIMIZATION: ‡∏ñ‡πâ‡∏≤ Good Matches ‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ RANSAC (‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏ß‡∏•‡∏≤)
        if len(good_matches) < 4: return 0
        
        # üöÄ OPTIMIZATION: ‡∏ñ‡πâ‡∏≤ Good Matches ‡πÄ‡∏¢‡∏≠‡∏∞‡∏û‡∏≠‡πÅ‡∏•‡πâ‡∏ß ‡∏ï‡∏≠‡∏ö‡πÄ‡∏•‡∏¢! (‡∏Ç‡πâ‡∏≤‡∏° RANSAC)
        # RANSAC ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Geometry ‡∏ß‡πà‡∏≤‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏° ‡∏ã‡∏∂‡πà‡∏á‡∏Å‡∏¥‡∏ô CPU
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (Raw Count) ‡∏Å‡πá‡∏û‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß
        if len(good_matches) > 15: 
            return len(good_matches) # ‡πÄ‡∏£‡πá‡∏ß‡∏™‡∏∏‡∏î‡πÜ

        # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡πâ‡∏≥‡∏Å‡∏∂‡πà‡∏á ‡∏Ñ‡πà‡∏≠‡∏¢‡πÉ‡∏ä‡πâ RANSAC ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ä‡πá‡∏Ñ
        src_pts = np.float32([kp_q[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp_db[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
        if mask is None: return 0
        
        return mask.ravel().tolist().count(1)